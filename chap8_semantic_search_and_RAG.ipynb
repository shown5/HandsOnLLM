{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN3YiqBBlcE+sl4q66hbt0j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shown5/HandsOnLLM/blob/main/chap8_semantic_search_and_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Colab では Python 3.12 ＋ numpy 1.26 系と faiss 1.8 系の組み合わせがうまく噛み合わないケースが報告されているらしい。\n",
        "%%capture\n",
        "# !pip install langchain==0.2.5 faiss-cpu==1.7.4 cohere==5.5.8 langchain-community==0.2.5 rank_bm25==0.2.2 sentence-transformers==3.0.1 numpy==1.24.4\n",
        "!pip install langchain==0.2.5 faiss-cpu==1.12.0 cohere==5.5.8 langchain-community==0.2.5 rank_bm25==0.2.2 sentence-transformers==3.0.1 numpy==1.24.4\n",
        "!pip install llama-cpp-python==0.2.78  --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu124"
      ],
      "metadata": {
        "id": "4VSaRBsw73rw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffc7a6c9",
        "outputId": "aa788048-90b6-4fff-9d2a-1d979102c22b"
      },
      "source": [
        "%pip install cohere"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cohere\n",
            "  Downloading cohere-5.18.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting fastavro<2.0.0,>=1.9.4 (from cohere)\n",
            "  Downloading fastavro-1.12.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: httpx>=0.21.2 in /usr/local/lib/python3.12/dist-packages (from cohere) (0.28.1)\n",
            "Collecting httpx-sse==0.4.0 (from cohere)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: pydantic>=1.9.2 in /usr/local/lib/python3.12/dist-packages (from cohere) (2.11.9)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.12/dist-packages (from cohere) (2.33.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from cohere) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.12/dist-packages (from cohere) (0.22.0)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere)\n",
            "  Downloading types_requests-2.32.4.20250913-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from cohere) (4.15.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.21.2->cohere) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.21.2->cohere) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.21.2->cohere) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.21.2->cohere) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.21.2->cohere) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9.2->cohere) (0.7.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9.2->cohere) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (2.5.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers<1,>=0.15->cohere) (0.35.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (1.1.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.21.2->cohere) (1.3.1)\n",
            "Downloading cohere-5.18.0-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.4/295.4 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading fastavro-1.12.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_requests-2.32.4.20250913-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: types-requests, httpx-sse, fastavro, cohere\n",
            "  Attempting uninstall: httpx-sse\n",
            "    Found existing installation: httpx-sse 0.4.1\n",
            "    Uninstalling httpx-sse-0.4.1:\n",
            "      Successfully uninstalled httpx-sse-0.4.1\n",
            "Successfully installed cohere-5.18.0 fastavro-1.12.0 httpx-sse-0.4.0 types-requests-2.32.4.20250913\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "buUPyr7g6Vn9"
      },
      "outputs": [],
      "source": [
        "import cohere\n",
        "from google.colab import userdata\n",
        "\n",
        "try:\n",
        "  api_key = userdata.get(\"COHERE_API_KEY\")\n",
        "except userdata.notebook_secret.NotebookSecretError:\n",
        "  print(\"Please store your Cohere API key in Colab secrets with the name 'COHERE_API_KEY'\")\n",
        "  api_key = None\n",
        "\n",
        "if api_key:\n",
        "  co = cohere.Client(api_key)\n",
        "else:\n",
        "  print(\"Cohere client could not be initialized due to missing API key.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "Interstellar is a 2014 epic science fiction film co-written, directed, and produced by Christopher Nolan.\n",
        "It stars Matthew McConaughey, Anne Hathaway, Jessica Chastain, Bill Irwin, Ellen Burstyn, Matt Damon, and Michael Caine.\n",
        "Set in a dystopian future where humanity is struggling to survive, the film follows a group of astronauts who travel through a wormhole near Saturn in search of a new home for mankind.\n",
        "\n",
        "Brothers Christopher and Jonathan Nolan wrote the screenplay, which had its origins in a script Jonathan developed in 2007.\n",
        "Caltech theoretical physicist and 2017 Nobel laureate in Physics[4] Kip Thorne was an executive producer, acted as a scientific consultant, and wrote a tie-in book, The Science of Interstellar.\n",
        "Cinematographer Hoyte van Hoytema shot it on 35 mm movie film in the Panavision anamorphic format and IMAX 70 mm.\n",
        "Principal photography began in late 2013 and took place in Alberta, Iceland, and Los Angeles.\n",
        "Interstellar uses extensive practical and miniature effects and the company Double Negative created additional digital effects.\n",
        "\n",
        "Interstellar premiered on October 26, 2014, in Los Angeles.\n",
        "In the United States, it was first released on film stock, expanding to venues using digital projectors.\n",
        "The film had a worldwide gross over $677 million (and $773 million with subsequent re-releases), making it the tenth-highest grossing film of 2014.\n",
        "It received acclaim for its performances, direction, screenplay, musical score, visual effects, ambition, themes, and emotional weight.\n",
        "It has also received praise from many astronomers for its scientific accuracy and portrayal of theoretical astrophysics. Since its premiere, Interstellar gained a cult following,[5] and now is regarded by many sci-fi experts as one of the best science-fiction films of all time.\n",
        "Interstellar was nominated for five awards at the 87th Academy Awards, winning Best Visual Effects, and received numerous other accolades\"\"\"\n",
        "\n",
        "# テキストを文単位に分割\n",
        "texts = text.split('.')\n",
        "\n",
        "# スペースや改行を除去\n",
        "texts = [t.strip(' \\n') for t in texts]"
      ],
      "metadata": {
        "id": "Q5n3WCcs7wR9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 埋め込みを取得\n",
        "response = co.embed(\n",
        "    texts=texts,\n",
        "    input_type=\"search_document\",\n",
        "    model=\"embed-multilingual-v3.0\"\n",
        ").embeddings\n",
        "\n",
        "embeds = np.array(response)\n",
        "print(embeds.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZnoQedzFCsk",
        "outputId": "d898955f-aec6-47c3-df1a-2fe0e966e7d2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(15, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "# import numpy as np\n",
        "\n",
        "# shape = (15, 1024)\n",
        "print(type(embeds), embeds.shape)\n",
        "\n",
        "# numpy の float32 に変換\n",
        "embeds_np = np.ascontiguousarray(np.array(embeds, dtype=np.float32))\n",
        "\n",
        "print(type(embeds_np), embeds_np.shape, embeds_np.dtype, embeds_np.flags['C_CONTIGUOUS'])\n",
        "# dtype=float32, C_CONTIGUOUS=True になっていることを確認\n",
        "\n",
        "\n",
        "dim = embeds_np.shape[1]\n",
        "index = faiss.IndexFlatL2(dim)\n",
        "\n",
        "print(index.is_trained)  # True のはず\n",
        "index.add(embeds_np)     # ここでエラーが消える"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "W7moU8QlH-Ov",
        "outputId": "239d40e6-98b0-4f7d-d03b-f7e286d47b76"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'faiss'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1385753656.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# import numpy as np\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# shape = (15, 1024)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'faiss'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def search(query, number_of_results=3):\n",
        "\n",
        "  # 1.クエリの埋め込みを取得\n",
        "  query_embed = co.embed(\n",
        "      texts=[query],\n",
        "      input_type=\"search_query\",\n",
        "      model=\"embed-multilingual-v3.0\" # ドキュメントと同じモデルを指定\n",
        "  ).embeddings[0]\n",
        "\n",
        "  # 2.最近傍を検索\n",
        "  distances, similar_item_ids = index.search(\n",
        "      np.float32([query_embed]),\n",
        "      number_of_results\n",
        "  )\n",
        "\n",
        "  # 3.結果を整形\n",
        "  # インデックス化を楽にするため、テキストのリストをNumPyの配列に変換\n",
        "  texts_np = np.array(texts)\n",
        "  results = pd.DataFrame(\n",
        "      data={\n",
        "          \"texts\": texts_np[similar_item_ids[0]],\n",
        "          \"distance\": distances[0]\n",
        "      }\n",
        "  )\n",
        "\n",
        "  # 4.結果を表示して返す\n",
        "  print(f\"Query:'{query}'\\nNearest neighbours:\")\n",
        "  return results"
      ],
      "metadata": {
        "id": "bp8QnrY0FuWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"科学的にはどれくらい正確だった？\"\n",
        "results = search(query)\n",
        "results"
      ],
      "metadata": {
        "id": "P-a6Tu_yN35w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41cbcc0b"
      },
      "source": [
        "%pip install rank_bm25"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sklearn.feature_extraction import _stop_words\n",
        "\n",
        "def bm25_tokenizer(text):\n",
        "  tokenized_doc = []\n",
        "  for token in text.lower().split():\n",
        "    token = token.strip(string.punctuation)\n",
        "\n",
        "    if len(token) > 0 and token not in _stop_words.ENGLISH_STOP_WORDS:\n",
        "      tokenized_doc.append(token)\n",
        "  return tokenized_doc"
      ],
      "metadata": {
        "id": "OoBpfMEmOIAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "tokenized_corpus = []\n",
        "for passage in tqdm(texts):\n",
        "  tokenized_corpus.append(bm25_tokenizer(passage))\n",
        "\n",
        "bm25 = BM25Okapi(tokenized_corpus)"
      ],
      "metadata": {
        "id": "Ba3zNvykPCOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def keyword_search(query, top_k=3, num_candidates=15):\n",
        "  print(\"Input question:\", query)\n",
        "\n",
        "  # BM25 を用いた検索\n",
        "  bm25_scores = bm25.get_scores(bm25_tokenizer(query))\n",
        "  top_n = np.argpartition(bm25_scores, -num_candidates)[-num_candidates:]\n",
        "  bm25_hits = [{'corpus_id': idx, 'score': bm25_scores[idx]} for idx in top_n]\n",
        "  bm25_hits = sorted(bm25_hits, key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "  print(f\"Top-3 lexical search (BM25) hits\")\n",
        "  for hit in bm25_hits[0:top_k]:\n",
        "    print(\"\\t{:3f}\\t{}\".format(hit['score'], texts[hit['corpus_id']].replace(\"\\n\", \" \")))"
      ],
      "metadata": {
        "id": "OpC5CWohg3B0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keyword_search(query=\"how precise was the science\")"
      ],
      "metadata": {
        "id": "nkwjCx6ti4HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8.2.2.1\n",
        "query = \"科学的にどれくらい正確だった？\"\n",
        "results = co.rerank(query=query, documents=texts, top_n=3, return_documents=True)\n",
        "results.results"
      ],
      "metadata": {
        "id": "FrPLMcEnVr6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, result in enumerate(results.results):\n",
        "  print(idx, result.relevance_score, result.document.text)"
      ],
      "metadata": {
        "id": "_LMjvpeNWD-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def keyword_and_reranking_search(query, top_k=3, num_candidates=10):\n",
        "    print(\"Input question:\", query)\n",
        "\n",
        "    ##### BM25 search (lexical search) #####\n",
        "    bm25_scores = bm25.get_scores(bm25_tokenizer(query))\n",
        "    top_n = np.argpartition(bm25_scores, -num_candidates)[-num_candidates:]\n",
        "    bm25_hits = [{'corpus_id': idx, 'score': bm25_scores[idx]} for idx in top_n]\n",
        "    bm25_hits = sorted(bm25_hits, key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "    print(f\"Top-3 lexical search (BM25) hits\")\n",
        "    for hit in bm25_hits[0:top_k]:\n",
        "        print(\"\\t{:.3f}\\t{}\".format(hit['score'], texts[hit['corpus_id']].replace(\"\\n\", \" \")))\n",
        "\n",
        "    #Add re-ranking\n",
        "    docs = [texts[hit['corpus_id']] for hit in bm25_hits]\n",
        "\n",
        "    print(f\"\\nTop-3 hits by rank-API ({len(bm25_hits)} BM25 hits re-ranked)\")\n",
        "    results = co.rerank(query=query, documents=docs, top_n=top_k, return_documents=True)\n",
        "    for hit in results.results:\n",
        "        print(\"\\t{:.3f}\\t{}\".format(hit.relevance_score, hit.document.text.replace(\"\\n\", \" \")))"
      ],
      "metadata": {
        "id": "IWegcVXbjfFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keyword_and_reranking_search(query = \"how precise was the science\")"
      ],
      "metadata": {
        "id": "tgGcSfIzQ_Bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 興行収入に関するクエリ\n",
        "query = \"income generated\"\n",
        "\n",
        "# 検索\n",
        "# 検索には埋め込みを使用する。ただし、理想的にはハイブリッド検索が良い\n",
        "results = search(query)\n",
        "\n",
        "# 検索結果を用いた生成\n",
        "docs_dict = [{\"text\": text} for text in results[\"texts\"]]\n",
        "response = co.chat(\n",
        "    message = query,\n",
        "    documents=docs_dict\n",
        ")\n",
        "\n",
        "print(respose.text)"
      ],
      "metadata": {
        "id": "5ogQTsuMUGjN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}